ulimit -s 65536


fine-tune from resnext101
https://github.com/banjuanshua/pytorch-finetune-example/blob/master/main.py

wideresnet

zoo
https://pypi.org/project/timm/

photometric distortions
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/blob/master/utils.py
http://www.telesens.co/2018/06/28/data-augmentation-in-ssd/#Photometric_Distortions

try ideas from here (extending the network w/ bn)
https://github.com/stratospark/food-101-keras/blob/master/food.py

test 
 - multi-scale


at 0.86 Hoorray!!

- try nesterov momemtum
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)


STEP 7: INSTANTIATE STEP LEARNING SCHEDULER CLASS
'''
# lr = lr * factor 
# mode='max': look for the maximum validation accuracy to track
# patience: number of epochs - 1 where loss plateaus before decreasing LR
        # patience = 0, after 1 bad epoch, reduce LR
# factor = decaying factor
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=0, verbose=True)


to change lr per the paper after x namy steps
https://stackoverflow.com/questions/48324152/pytorch-how-to-change-the-learning-rate-of-an-optimizer-at-any-given-moment-no


use the train-test split of the daatset (instead of your own)

diff between learning-rate decay and weight decay
https://discuss.pytorch.org/t/how-does-sgd-weight-decay-work/33105/2
